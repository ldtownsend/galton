{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de6f2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2674f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow as pa\n",
    "\n",
    "import duckdb\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931c303a",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "The general process for updating these is to:\n",
    "\n",
    "0) Read configs for each dataset\n",
    "1) Read in the data already collected and stored in partitions (use duckdb for reads)\n",
    "    - Get the data for the `n_latest_dates`\n",
    "    - `df_current`: to use later for filtering out duplicate data already stored\n",
    "2) Load all new data parquet files and concatenate.\n",
    "3) Perform all of the requisite preprocessing steps for that type of data\n",
    "4) Filter out duplicates already in `df_current`\n",
    "5) Save using pd.to_parquet() with partition_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42eee4d",
   "metadata": {},
   "source": [
    "# 0 - Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82589460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.galton.data_collection.partition_configs import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8ffd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_latest_dates = 5\n",
    "data_path = \"/Users/luketownsend/Desktop/projects/tetlock/data/\"\n",
    "\n",
    "select_dataset = \"openmeteo_forecasts\"\n",
    "\n",
    "partition_folder_name = config[select_dataset][\"partition_folder_name\"]\n",
    "start_date_field = config[select_dataset][\"start_date_field\"]\n",
    "new_data_file_prefixes = config[select_dataset][\"new_data_file_prefixes\"]\n",
    "record_index_fields = config[select_dataset][\"record_index_fields\"]\n",
    "output_columns = config[select_dataset][\"output_columns\"]\n",
    "partition_columns = config[select_dataset][\"partition_columns\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c681e878",
   "metadata": {},
   "source": [
    "# 1 - Read Existing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217ad898",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect()\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {partition_folder_name} AS\n",
    "    SELECT *\n",
    "    FROM read_parquet('data/local_data/{partition_folder_name}/**/*.parquet');\n",
    "\"\"\")\n",
    "\n",
    "query = f\"\"\"\n",
    "WITH latest_dates AS (\n",
    "    SELECT DISTINCT {start_date_field}\n",
    "    FROM {partition_folder_name}\n",
    "    ORDER BY {start_date_field} DESC\n",
    "    LIMIT {n_latest_dates}\n",
    ")\n",
    "SELECT f.*\n",
    "FROM {partition_folder_name} f\n",
    "JOIN latest_dates d\n",
    "USING ({start_date_field})\n",
    "\"\"\"\n",
    "\n",
    "con.execute(query)\n",
    "df_current = con.fetch_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b001de43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_current[start_date_field].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd5b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_current.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4457fb0",
   "metadata": {},
   "source": [
    "# 2 Load New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9fdab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "#TODO: Modify to accept a list of prefixes\n",
    "#TODO: Write function to convert date to start_date format\n",
    "def generate_prefixed_dates(prefix: str, start_date: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Given a prefix (e.g. 'multi_model_forecast - ') and a start date ('2025-11-21'),\n",
    "    return a list of 'prefix + YYYY-MM-DD' for every date from start_date through today.\n",
    "    \"\"\"\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
    "    today = datetime.today().date()\n",
    "    \n",
    "    num_days = (today - start).days + 1\n",
    "    return [f\"{prefix}{(start + timedelta(days=i))}\" for i in range(num_days)]\n",
    "\n",
    "\n",
    "\n",
    "start_date = df_current[start_date_field].min()\n",
    "\n",
    "files = generate_prefixed_dates(new_data_file_prefixes[0], start_date)\n",
    "print(files[-5:])  # show last few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cfe41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ds.dataset(data_path, format=\"parquet\")\n",
    "\n",
    "filenames = dataset.files\n",
    "filtered = [f for f in filenames if any(p in f for p in files)]\n",
    "print(len(filtered))\n",
    "\n",
    "frames = []\n",
    "for file in filtered:\n",
    "    temp_df = pd.read_parquet(file)\n",
    "    frames.append(temp_df)\n",
    "\n",
    "df = pd.concat(frames)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956dfb34",
   "metadata": {},
   "source": [
    "# 3 - Preprocess New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3582af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Refactor & move into dedicated module or add to data_collection.openmeteo?\n",
    "\n",
    "from src.galton.data_collection.utilities import (\n",
    "    normalize_field_names,\n",
    "    convert_datetime_to_utc,\n",
    ")\n",
    "from src.galton.feature_engineering.dates import add_date_fields\n",
    "from src.galton.feature_engineering.forecasts import (\n",
    "    add_forecast_fields,\n",
    "    filter_redundant_forecasts,\n",
    "    filter_unused_forecast_data,\n",
    ")\n",
    "\n",
    "df = normalize_field_names(df)\n",
    "df = add_date_fields(df)\n",
    "df = add_forecast_fields(df)\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df = filter_unused_forecast_data(df)\n",
    "df = filter_redundant_forecasts(df)\n",
    "\n",
    "df = convert_datetime_to_utc(df)\n",
    "\n",
    "df[\"forecast_horizon\"] = df[\"forecast_horizon\"].astype(int)\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "datetime_cols = [\"datetime\", \"model_timestamp\", \"current_timestamp\"]\n",
    "\n",
    "for col in datetime_cols:\n",
    "    df[col] = df[col].dt.tz_convert(\"America/Chicago\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d5697b",
   "metadata": {},
   "source": [
    "# 4 - Filter Existing Records from New Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ecaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_index = df_current.set_index(record_index_fields).index\n",
    "\n",
    "df = df.set_index(record_index_fields)\n",
    "\n",
    "df = df[~df.index.isin(current_index)].reset_index()\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df = df[output_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad13af8e",
   "metadata": {},
   "source": [
    "# 5 - Save New Data to Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a882effc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(f\"data/local_data/{partition_folder_name}\", engine=\"pyarrow\", partition_cols=partition_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e04d762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbce429",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
